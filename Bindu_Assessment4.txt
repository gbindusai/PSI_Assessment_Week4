#Code1
twitteragent.sources = source1
twitteragent.sinks = sink1

twitteragent.channels = channel1
twitteragent.sources.source1.channels = channel1
twitteragent.sinks.sink1.channel = channel1

twitteragent.sources.source1.type = org.apache.flume.source.twitter.TwitterSource
twitteragent.sources.source1.consumerKey = nnLUJzJhh8086yZt5rF9V2YQj
twitteragent.sources.source1.consumerSecret = IKUM5IseblFRpSNrNoyh5EWVDlkI3zu6wRcC9ttVlYPiH8lR1f
twitteragent.sources.source1.accessToken = 1494474292129529858-6DRAh82ILudjSoRDArqnAHMhVTeg5C
twitteragent.sources.source1.accessTokenSecret = RDVRWeEODszRVkaGeEY9dOXctXKXXYarJj1G1zRefDjRh
twitteragent.sources.source1.keywords = @covid-19 

twitteragent.sinks.sink1.type = hdfs
twitteragent.sinks.sink1.hdfs.path = /flume/twitter
twitteragent.sinks.sink1.hdfs.filePrefix = events
twitteragent.sinks.sink1.hdfs.fileSuffix = .log
twitteragent.sinks.sink1.hdfs.inUsePrefix = _
twitteragent.sinks.sink1.hdfsfiletype = DataStream

twitteragent.channels.channel1.type = memory
twitteragent.channels.channel1.capacity = 1000


#Code2
sqoop import \
--connect jdbc:mysql://localhost:3306/PetsDb?useSSL=false \
--username=root \
--password=password \
--table=pet \
--hive-import \
--hive-table=pet_direct \
--target-dir /mysql/table/pet_direct \
--m 1


#Code3
flightsPath = "hdfs:///spark/rdd/flights.csv"
flightsData = sc.textFile(flightsPath)
flightData.take(10)
flightsData.collect()
flightsData.count()
flightsData.first()


#Code4
twitterPath = "hdfs:///spark/sql/cache-0.json"
import json
from pyspark.sql import SQLContext,Row
sqlC = SQLContext(sc)
twitterTable = sqlC.read.json(twitterPath)
twitterTable.registerTempTable("twitTab")
sqlC.sql("Select text, user.screen_name from twitTab where user.screen_name='realDonaldTrump' limit 10").collect()


#Code5
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
sc = SparkContext("local[2]", StreamingErrorCount")
spark_sc = StreamingContext(sc,10)
spark_sc.checkpoint("hdfs:///spark/streaming")
ds1 = spark_sc.socketTextStream("localhost",9999)
count = ds1.flatMap(lambda x:x.split(" ")).filter(lambda word:"ERROR" in word).map(lambda word:(word,1)).reduceByKey(lambda x,y:x+y)
count.pprint()
spark_sc.start()
spark_sc.awaitTermination()


#Code6
flightsPath = "hdfs:///spark/rdd/flights.csv"
flightsData = sc.textFile(flightsPath)

def notHeader(row):
    return "AIRLINE_ID" not in row

import csv
from StringIO import StringIO
def split(line):
    reader = csv.reader(StringIO(line))
    return reader.next()

from datetime import datetime
from collections import namedtuple
fields = ('date','airline','flightnum','origin','dest','dep','dep_delay','arv','arv_delay','airtime','distance')
Flight  = namedtuple('Flight',fields, verbose=False)
DATE_FMT = '%Y-%m-%d'
TIME_FMT = '%H%M%S'
def parse(row):
    row[0] = datetime.strptime(row[0], DATE_FMT).date()
    row[5] = datetime.strptime(row[5], TIME_FMT).time()
    row[6] = float(row[6])
    row[7] = datetime.strptime(row[7], TIME_FMT).time()
    row[8] = float(row[8])
    row[9] = float(row[9])
    row[10] = float(row[10])
    return Flight(*row[:11])

finalflight = flightsData.map(lambda x:','.join(x or '00.00' for x in x.split(','))).map(lambda x:x.replace(',""',',"0000"')).filter(notHeader).map(split).map(parse)
airportDelays = finalflight.map(lambda x: (x.origin,x.dep_delay))
airportTotalDelay = airportDelays.reduceByKey(lambda x,y:x+y)
airportCount = airportDelays.mapValues(lambda x:1).reduceByKey(lambda x,y:x+y)
airportsSumCount = airportTotalDelay.join(airportCount)
airportAvgDelay=airportsSumCount.mapValues(lambda x:x[0]/float(x[1]))
airportAvgDelay.sortBy(lambda x:-x[1]).take(10)







